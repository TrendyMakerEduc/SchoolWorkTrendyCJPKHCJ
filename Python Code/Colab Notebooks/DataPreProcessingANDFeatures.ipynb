{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO6CQuoxu0j+SjSdy2VvmD7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"MaGFmRnBxMKr","executionInfo":{"status":"ok","timestamp":1664970005163,"user_tz":420,"elapsed":9,"user":{"displayName":"GameMusicEnthusiast","userId":"15069885440578359632"}}},"outputs":[],"source":["##DATA PRE-PROCESSING\n","##MidTerm- October 17th- Go over all documents, no code on the midterm, equations are on the midterm, but you will not be told what\n","#they are, so remember key points. Midterm will be in another classroom.\n","\n","#Data Preprocessing\n","#Everything done wotj tje data prior to the application of a model on it\n","#-Aggregation- Trying to combine features with fewer features\n","#-Sampling- Pull from different groups/pieces of data\n","#-Discretization and Binarization- Continous to more discrete attributes\n","#-Dimensionality Redudtion\n","#-Subset- Finding out what is useful and what is not\n","#-Feature Creation- create our own features/new features that are useful\n","\n","#Aggregiation\n","#-Combinging two or more attributes (or objects) into a single attribute (or object)\n","#Example-Item substituted intp transaction ID\n","#Purpose- Data Reduction (reduced rows/samples or attributes), change the scale (ex: every city where the purchase came from,\n","# where the data only pops up once, there is not much correlation, change it to every province/state or to time as well.)\n","#-more stable data (we have less variation in our possible values, ex: cities have more options, provinces get grouped together)\n","\n","#Sampling\n","#Sampling is the main technique employed for data reduction\n","#-It is often used for both the preliminary investigation of the data and the final data analysis.\n","#Easier to work with smaller data.\n","#Statisiticians often sample because obtaining the entire set of data of interest is too expensive or time consuming\n","#The key principle for effective sampling is the following:\n","#-Using a sample will work almost as well as using the entire data set, if the sample is representative\n","#-A sample is representative if it has approximately the same properties (of interest) as the original set of data\n","#Is this sample considered representative of the dataset?\n","#-Height of cats to estimate the height of mammals\n","#How to get a representative sample of the heights of mammmals\n","#1)Measure x of each type\n","#2)(not so useful)- Random\n","#Simple Random Sampling\n","#-There is an equal probability of selecting any particular item\n","#-Sampling without replacement\n","#  -As each item is selected, it is removed from the population\n","#-Sampling with replacement\n","#   -Objects are not removed from the population as they are selected for the sample.\n","#   -In sampling with replacement, the same object can be picked up more than once\n","#Stratified sampling\n","#-Split the data into several partitions; then draw random samples from each partition\n","#YOU WANT TO SEE A PATTERN, AND THEN DO THIS\n","#PROOGRESSIVE SAMPLING\n","#-The proper sample size can be difficult to determine, so adaptive or progressive sampling schemes are sometimes used.\n","#These approaches start with a small sample, and then increase the sample size until a sample of sufficient size has been obtained. While this technique \n","#eliminates the eneed to determine the correct sample size initially, it requires that there be a way to evaluate the samople to judge if it is large enough\n","\n","#Features == attributes\n","#Discretization\n","#Discretization is the process of converting a continous attribute into an ordinal attribute\n","#-A potentially infinite number of values are mapped into a small number of categories\n","#-Discretization is used in both unsupervised and supervised settings\n","#Unsupervised- data does not contain labels\n","#Supervised- Data does contain labels\n","\n","#Simple way- Binarization\n","#-A simple technique to binarize a categorical attribute is the following:\n","#1)If there are m categorical values, then uniquely assign each original value to an integer in the inteveal (0, m-1)\n","#CONT\n","\n","#EXAMPLE- awful, poor, ok, good, great, assigned integer values and counting how many times with 0 and 1,\n","# for every categorical feature\n","\n","##REMEMBER- SMC = number of matches/number of attributes or = (f11 + f01) / (f01 + f10 + f11 + f00)\n","#J = number of 11 matches/number of non-zero attributes or = (f11) / (f01 + f10 + f11)\n","#cos(d1, d2) = <d1,d2> / ||d1|| ||d2||\n","\n","# 4 00, #3 01, 1 11, 3 10\n","\n","#1 + 3 / 3 + 3 + 1 + 4\n","#4 / 11 for example in SMC for all values\n","#for good and poor\n","#2 / 3\n","# 0 \n","\n","#1 / 3 + 3 + 1\n","#1/7 for example in J for all values\n","#for good and great\n","# 1 / 3\n","# 0\n","\n","# good is closer to poor the great\n","# 1/2\n","# 0\n","#Similarity in binary vectors breaks\n","\n","#This isn't our only binarization\n","#One-hot encoding\n","#Every attribute is correlated to one attribute to our ordinal attribute\n","#We don't make assumptions, we keep track of order is how far the vector occurs\n","#More attributes = more complexity\n","\n","#We want to start grouping values\n","\n","#Unsupervised Discretization\n","#-Binning\n","#  -Equal width- THe distance between groups is equal\n","#  bins are defined as [min,min + w), [min + w, min + 2w).... [min +(n+1)w, min + nw)\n","# where]]]\n","# w= (max-min)/(number of bins)\n","#Equal frequency- THe number of samples in each group is equal\n","#Age = [5, 10, 11, 13, 15, 35, 50, 55, 72, 92, 204, 215) into 3 bins]\n","#Equal width\n","#w= (215-5)/3=70\n","\n","#Exercise\n","# (70-10)/ 4 = 15 = w\n","#We go up by our bin size from the lowest\n","#bin 1 (10, 25)\n","#bin 2 (25, 40)\n","#bin 3 (40, 55)\n","#bin 4 (55, 70)\n","# These are the bin ranges\n","#Equal distance\n","\n","#Equal frequency\n","# There are 9 values/ 4 total bins = 2 remainder 1\n","#Bin 1 = [10, 20]\n","#Bin 2 = [35, 40]\n","#Bin 3 = [44, 60]\n","#Bin 4 = [65, 66] remainder 1 so [65, 66, 70] to backfill if there is a remainder of 1, if remainder 2 we need to backfill two\n","#Everything is in order, all values maintain order\n","#Equal frequency gets the number of values per bin in order\n","\n","#K-means- uses k-means clustering to define groups\n","#K-means clustering:\n","#1)Randomly select k data points (centroids)\n","#2) For each data point a) Find the nearest centroid and add the data point to that cluster\n","#3) Recalculate the centroids\n","#4) Repeat steps 2-4 until the centroids do not change\n","#centroid = center of cluster\n","#You find the points closest to these points, and then redefine these as clusters. After, use these clusters, and make new centroids\n","#Then, go back to the first step, to redefine clusters, until the centroid through iterations do not change, and you have\n","#your final clusters\n","#Then use cosine similarity, or any similarity with this algorithm\n","\n","#You can perform equal distance or equal frequency to shift them into bins\n","#These are unsupervised approaches\n","\n","#Now\n","#Supervised Discretization\n","#We have labels\n","#We want to categorize the samples in a way the all samples in a category have the same label\n","#Entropy-based approaches are one of the most promising approaches to discretization, whether bottom-up or top-down.\n","#Intuitively, the entropy of an interval is a measure of the purity of an interval. If an interval contains only values of one class (is perfectly pure)\n","#, then the entropy is 0 and it contributes nothing to the overall entropy. If the classes of values in an interval occur equally often (the interval)\n","#is as impure as possible), then the entropy is a maximum.\n","\n","#Categorical attributes with too many values\n","#Domain-based knowledge\n","# -Towns->counties->provinces\n","#If no exterior knowledge is avialable then measure the change in performance when grouping some together\n","\n","#Attribute transformation\n","#Attribute transform- a function that maps the entire set of values of a given attribute to a new set of replacement values such that each old\n","#value can be identified with one of the new values\n","#Simple function x^k, log(x), e^x, |x|\n","#-Suppose the variable of interest is the number of data bytes in a session, and the number of bytes ranges from 1 to 1 billion\n","#This is a huge range, and it can be advantageous to comporess it by using a log10 transformation. In this case, sessions that transferred 10^8 and 10^9 bytes\n","#would be more similar to each other than sessions that transferred 10 and 1000 bytes\n","\n","#Attribute functions\n","#Simple functions: x^k, log(e), e^x, |x|\n","# for 1/x, values less then 1 would not be good for transforms due to them being greater then 1.\n","#Standarization and normalization\n","#- The goal of standarization or normalization is to make an entire set of values have a particular property. A traditional example is that of \"standardizing a variable\" in statistics\n","#- If x is the mean (average) of the attribute values and s(x) is their standard deviation, then the transformation x^1 = (x- x(avg))/s(x)\n","\n","#Dimensionality reduction\n","#- Mapping from a vector space with high dimensionality to a vector space with much fewer dimensions.\n","#- High dimensionality\n","#  - Representing a document by the frequency of words\n","#  - Daily information over 100 years.\n","#Purpose\n","#- Avoid curse of dumensionality\n","#  -Data becomes too sparse\n","#-Reduce amount of time and memory required by data mining algorithms\n","#- Allow data to be more easily visualized\n","#-May help to eliminate irrelevant features or reduce noise\n","#Techniques\n","#- Principle Components Analysis (PCA)\n","#- Singular Value Decomposition\n","#-Others: supervised and non-linear techniques\n","#When dimensionality increases, data becomes increasingly sparse in the space that is occupies\n","#Definitions of density and distance between points, which are critical for clustering and outlier detection, become less meaningful\n","#PCA- finding a function to reduce dimensions, for example, a function that brings points to go in a linear way.\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"yXzZCp5X8ffu"},"execution_count":null,"outputs":[]}]}