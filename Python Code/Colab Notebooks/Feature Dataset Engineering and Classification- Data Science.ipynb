{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOr08gMB3TtJdigZHJLDhuE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"uqtnJfYMccr1"},"outputs":[],"source":["#Feature subset selection (feature engineering)\n","#- Another way to reduce dimensionality of data\n","#- Remove all \"useless\" features (only keep features that will help us)\n","#- Redundant features\n","#  -Duplicate much or all of the information contained in one or more other attributes\n","#    -purchase price of a product and the amount of sales tax paid\n","#- Irrevlevant features\n","#  - COntain no information that is useful for the data mining task at hand\n","#    - student's ID is often irrelevant to the task of predicting students' GPA\n","#  - Some features may seem irrelevant but can still provide information\n","#    - Temperature in Melbourne, Australia to predict ice cream sales in Antigonish\n","\n","#Feature engineering methods\n","#1) Have the feature selection embedded in the model\n","#- Decision trees\n","#- Artificial neural networks\n","#2) Filter\n","#- Apply a filter that only retains features that fit some criteria\n","#   - E.g. minimizing pairwise correlation\n","#3) (Wrapper) Leave-one-out\n","#-Try all possible combination of features\n","#-Leave one out\n","#  -Evaluate model using all features except one and compare performance\n","#  -If performance decreased, then that feature was important\n","\n","#What neural networks do- Feature weighting\n","#-Some features are more important that others\n","#- We can assign weights to those important features so that they have a larger affect on our model\n","#- Weighting can replace the use of removing a feature\n","#- This is the basis of artifical neural networks!\n","\n","#Feature creation\n","#- Create new attributes that can capture the important information in a data set much more efficently that the orginal attributes\n","#Two general methodologies\n","#- Feature extraction\n","#   - Extracting edges from images\n","#-Mapping data to new space\n","#  -Sparse to dense vectors\n","\n","#CLASSIFICATION\n","#Assign a label/class 'y' to a sample based on its features\n","#  - Learn a relationship/mapping \"f(x)\" from inputted features to a set of labels.\n","#  - Model classified correctly if f(x) = y\n","#-Label is categorical\n","#-Features can be any type\n","\n","##EXAMPLES\n","#Spam filtering- binary\n","#- Features extracted from email message header and content\n","#label- spam or not spam\n","#Tumor identification- binary\n","#-Features extracted from magnetic resonance imaging (MRI) scans\n","#-malignant or benign\n","#Galaxy Identification- discrete\n","#-Features extracted from telescope images\n","#-ellipitcal- spiral- or irregular\n","\n","#Classification roles\n","#Predicitive model- classify unlabeled samples\n","#- Accurate classifications\n","#- Perform classification quickly\n","#- Descriptive model- Identify the characteristics that distinguish instances from different classes\n","#For example, a classification model induced from the vertebrate data set shown in Table 3.2 can be used to predict the class label of the following vertebrate\n","#In addition, it can be used as a descriptive model to help determine characteristics that define a vertebrate as a mammal, a reptile, a bird, a fish, or an amphibian\n","\n","#General approach for building a classification model\n","#Training set induces knowledge, gather as much info as possible, so later on we can predict accurately\n","#We take the model and induce a learning algorithm, and we deduced a test set for labels.\n","#We tune in a development set to tune in the learning algorithm to make more accurate results, and so it gets better on the dev set\n","#With the dev set, we want to tune in the generalization to generate new data\n","#Dev set before test set.\n","\n","#Fair evaluation\n","#- The classifier should not see the test set prior to evaluations.\n","#- Training set is often much larger than the test set\n","#-80/20 train/test\n","#-70/10/20 train/dev/test\n","#-Classifier model is evaluated by comparing predicted labels against true labels\n","\n","#Confusion Matrix\n","#Each extry f(1j) denotes the number of instances from class i predicted to be of class j.\n","#For example, f(01) is the number of instances from class 0 incorrectly predicted as class 1.\n","#The number of correct predicitions made by the model is (f11 + f00) and the number of incorrect predictions is (f10 + f01).\n","#We can start calculating extra information and more from this model.\n","\n","#Evaluation metrics\n","#-Binary labels -> assign one class as being positive and other as negative\n","#-(tp) True positive = # of predicted positive that have true label as positive\n","#-(tn) True negative = # of predicted negative that have true label as negative\n","#-(fp) False positive= # of predicted positive that have true label as negative.\n","#-(fn) False negative- # of predicted negative that have true label as positive.\n","\n","#Evaluation metrics\n","#- ACCURACY = tp + tn / tp + tn + fp + fn (fraction of correctly labeled)\n","#- ERROR RATE = 1 - accuracy\n","#- PRECISION = tp/tp + fp (Fraction of positive predictions that are correct) (CONFIDENCE IN ANSWER)\n","#- RECALL = tp / tp + fn (fraction of actual postive that were correctly labelled) (AS MANY POSITIVES AS POSSIBLE)\n","\n","#Unfair accuracy\n","#- Dataset : rain or not rain\n","#  -Test set\n","#    - 3 instances of rain (p) true positive\n","#    - 97 instances of not rain (f) false\n","#Accuracy is not always the best\n","#F-score/F-measure\n","#Harmonic mean of precision and recall\n","# F = 2 * (prec * recall/ prec + recall)\n","#Calculate F-score for rain predictor\n","# Always predict not rain\n","# it ends in 0.28, which is terrible actually\n","\n","#Excercise\n","#precision = 45 / 45 + 23 = 0.6617\n","#recall = 45 / 45 + 8 = 0.84\n","#F = 2 * (0.66 * 0.84/ 0.66 + 0.84) = 0.55 / 1.5\n","#F = 0.73 (if 0.849 it would be 0.744)\n","\n","## 50 50 models are fine for accuracy.\n","\n","\n"]}]}