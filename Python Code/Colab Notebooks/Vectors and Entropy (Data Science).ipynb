{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPOA3QgevICNrVWhIHkO1HB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qC4eEoVaa90q"},"outputs":[],"source":["## SMC versus Jaccard: Example\n","# x = 1 0 0 0 0 0 0 0 0 0 \n","# y = 0 0 0 0 0 0 1 0 0 1\n","\n","#SMC = number of matches / number of attributes # Simple Matching Coefficent\n","# = (f11 + f00) / (f01 + f10 + f11 + f00)\n","# J = number of 11 matches / number of non-zero attributes\n","# = (f11) / (f01 + f10 + f11)\n","\n","## ABOVE REQUIRES 1s and 0s\n","\n","#Cosine simlilarity\n","#- Commonly used with vector representations of words\n","#- Easy to visualize\n","#- Ignores magnitude\n","#- if0000000000000 d1 and d2 are two vectors, then\n","# cos(d1, d2)000000 = <d1,d2>/ ||d1|| ||d2||,\n","# where <d1, d2> indicates inner product or dot product of d1 and d2, and ||d|| is the length of vector d.\n","# ||d|| = square root <d, d>\n","\n","#Example\n","# A = [2, 3, 5]\n","# B = [1, 2, 1]\n","#<A, B> = (2 x 1) + (3 x 2) + (5 x 1) == 13\n","#Above is a dot product example\n","\n","# ||A|| example\n","# ||A|| = square root of 2 squared + 3 squared + 5 squared\n","# ||A|| = square root of 4 + 9 + 25\n","# ||A|| = 6.16\n","\n","# example of cosine similarity\n","# d1 = 3 2 0 5 0 0 0 2 0 0\n","# d2 = 1 0 0 0 0 0 0 1 0 2\n","\n","# <d1, d2> = (3 x 1) + (2 x 0) + (0 x 0) + (5 x 0) + (0 x 0) + (0 x 0) + (0 x 0) + (2 x 1) + (0 x 0) + (0 x 2)\n","# <d1, d2> = 3 + 0 + 0 + 0 + 0 + 0 + 0 + 2 + 0 + 0\n","# <d1, d2> = 5\n","\n","# ||d1|| = 3 squared + 2 squared + 5 squared + 2 squared\n","# ||d1|| = 9 + 4 + 25 + 4\n","# ||d1|| = square root of 42\n","# ||d1|| = 6.4802\n","\n","# ||d2|| = 2.45 (simplicity, instead of calculating)\n","\n","#cos(d1, d2) = 5 / (6.4805 x 2.45)\n","# = 0.315 is the answer\n","\n","#WE MUST REQUIRE BINARY VECTORS TO MAKE THE CALCULATIONS\n","#What information can we include in a binary vector?\n","# Example- yes or no answers, storing algorithms with 2 options, true or false, storing data with two options\n","#We only care about what the vector is pointing to,  cosine measures the angle between the vectors\n","#Jicard only cares about the 11s\n","\n","#INFORMATION AND PROBABILITY\n","#- Information Theory\n","# - Information relates to possible outcomes to an event\n","#    - Transmission of a message, flip of a coin, or measurement of a piece of data\n","# - The more certain an outcome , the less information that it contains and vice-versa\n","#   - For example, if a coin has two heads, then an outcome of heads provides no information\n","#   - More quantitatively, the information is related the probablility of an outcome\n","#     - THe smaller the probability of an outcome, the more information it provides and vice-versa\n","#- Entropy is the commonly used measure\n","\n","#Entropy- measurement of disorder\n","#- For\n","#  - a variable (event), X,\n","#  - with n possible values (outcomes), x1, x2, xn\n","#  - each outcome having probability, p1, p2, pn\n","# - the entropy of X, H(X), is given by \n","# (cont)\n","\n","# For a coin with probability p of heads and prbability  q = 1 - p of tails\n","# H = -plog2p - qlog2q\n","#For p = 0.5 = 0.5 (fair coin)\n","# H = -0.5log2 0.5 - 0.5 log2 0.5\n","# H = -0.5(-1) - 0.5(-1)\n","# H = 1\n","#For p = 1 or q = 1, H = 0\n","\n","#Mutual information\n","#Information one variable provides about another\n","#- Formally 1(X, Y) = H(X) + H(Y) - H(X, Y), where\n","# H(X,Y) is the joint entropy of X and Y,\n","#H(X,Y) = - sigma i sigma j p (ij)log2 p(ij)\n","# Where p(ij) is the probability that the ith value of X and the jth values of Y occur together\n","#- For discrete variables, this is easy to compute\n","\n","#Measuring the entropy\n"]}]}